{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6973661,"sourceType":"datasetVersion","datasetId":4006724}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-15T13:36:13.024198Z","iopub.execute_input":"2023-11-15T13:36:13.025055Z","iopub.status.idle":"2023-11-15T13:36:13.398301Z","shell.execute_reply.started":"2023-11-15T13:36:13.025016Z","shell.execute_reply":"2023-11-15T13:36:13.397350Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/europarl-dataset/europarl-v7.de-en.en\n/kaggle/input/europarl-dataset/europarl-v7.de-en.de\n","output_type":"stream"}]},{"cell_type":"code","source":"# import tarfile\n\n# # Replace 'your_file.tgz' with the actual file name\n# file_path = '/kaggle/input/europarl-dataset/de-en.tgz'\n\n# # Extract the contents of the tarball\n# with tarfile.open(file_path, 'r:gz') as tar:\n#     tar.extractall(path='/kaggle/working/')\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:36:13.400438Z","iopub.execute_input":"2023-11-15T13:36:13.400832Z","iopub.status.idle":"2023-11-15T13:36:13.405306Z","shell.execute_reply.started":"2023-11-15T13:36:13.400804Z","shell.execute_reply":"2023-11-15T13:36:13.404388Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"file_path = '/kaggle/input/europarl-dataset/europarl-v7.de-en.de'\n\n# Read the contents of the file into a list\nwith open(file_path, 'r', encoding='utf-8') as file:\n    eng = file.readlines()\n\n# Display the first few lines of the list\nprint(eng[:5])\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:36:13.406407Z","iopub.execute_input":"2023-11-15T13:36:13.406720Z","iopub.status.idle":"2023-11-15T13:36:18.784076Z","shell.execute_reply.started":"2023-11-15T13:36:13.406696Z","shell.execute_reply":"2023-11-15T13:36:18.783044Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"['Wiederaufnahme der Sitzungsperiode\\n', 'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.\\n', 'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.\\n', 'Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen.\\n', 'Heute möchte ich Sie bitten - das ist auch der Wunsch einiger Kolleginnen und Kollegen -, allen Opfern der Stürme, insbesondere in den verschiedenen Ländern der Europäischen Union, in einer Schweigeminute zu gedenken.\\n']\n","output_type":"stream"}]},{"cell_type":"code","source":"len(eng)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:36:18.785221Z","iopub.execute_input":"2023-11-15T13:36:18.785527Z","iopub.status.idle":"2023-11-15T13:36:18.791898Z","shell.execute_reply.started":"2023-11-15T13:36:18.785502Z","shell.execute_reply":"2023-11-15T13:36:18.790877Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"1920209"},"metadata":{}}]},{"cell_type":"code","source":"file_path = '/kaggle/input/europarl-dataset/europarl-v7.de-en.en'\n\n# Read the contents of the file into a list\nwith open(file_path, 'r', encoding='utf-8') as file:\n    ger = file.readlines()\n\n# Display the first few lines of the list\nprint(ger[:5])","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:36:18.794888Z","iopub.execute_input":"2023-11-15T13:36:18.795146Z","iopub.status.idle":"2023-11-15T13:36:23.277875Z","shell.execute_reply.started":"2023-11-15T13:36:18.795123Z","shell.execute_reply":"2023-11-15T13:36:23.276931Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"['Resumption of the session\\n', 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\n', \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\n\", 'You have requested a debate on this subject in the course of the next few days, during this part-session.\\n', \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\\n\"]\n","output_type":"stream"}]},{"cell_type":"code","source":"len(ger)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:36:23.279096Z","iopub.execute_input":"2023-11-15T13:36:23.279485Z","iopub.status.idle":"2023-11-15T13:36:23.286221Z","shell.execute_reply.started":"2023-11-15T13:36:23.279451Z","shell.execute_reply":"2023-11-15T13:36:23.285311Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"1920209"},"metadata":{}}]},{"cell_type":"code","source":"e = eng[:10000]\ng = ger[:10000]","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:36:23.287266Z","iopub.execute_input":"2023-11-15T13:36:23.287613Z","iopub.status.idle":"2023-11-15T13:36:23.297049Z","shell.execute_reply.started":"2023-11-15T13:36:23.287581Z","shell.execute_reply":"2023-11-15T13:36:23.296174Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nimport string\n\n# Download NLTK resources (if not already downloaded)\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\n\n# Function for text preprocessing\ndef preprocess_text_en(data):\n    # Convert to lowercase\n    data2 = []\n    for text in data:\n        text = text.lower()\n\n        # Remove punctuation and digits\n#         text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n\n        # Tokenization\n        tokens = word_tokenize(text)\n\n        # Remove stop words\n        stop_words = set(stopwords.words('english'))\n        tokens = [word for word in tokens if word not in stop_words]\n\n        # Lemmatization\n    #     lemmatizer = WordNetLemmatizer()\n    #     tokens = [lemmatizer.lemmatize(word) for word in tokens]\n\n        # Join the tokens back into a string\n        preprocessed_text = ' '.join(tokens)\n        data2.append(preprocessed_text)\n\n    return data2\n\ndef preprocess_text_gr(data):\n    # Convert to lowercase\n    data2 = []\n    for text in data:\n        text = text.lower()\n\n        # Remove punctuation and digits\n#         text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n\n        # Tokenization\n        tokens = word_tokenize(text)\n\n        # Remove stop words\n        stop_words = set(stopwords.words('german'))\n        tokens = [word for word in tokens if word not in stop_words]\n\n        # Lemmatization\n    #     lemmatizer = WordNetLemmatizer()\n    #     tokens = [lemmatizer.lemmatize(word) for word in tokens]\n\n        # Join the tokens back into a string\n        preprocessed_text = ' '.join(tokens)\n        data2.append(preprocessed_text)\n\n    return data2\n\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:36:23.298220Z","iopub.execute_input":"2023-11-15T13:36:23.299096Z","iopub.status.idle":"2023-11-15T13:36:25.026141Z","shell.execute_reply.started":"2023-11-15T13:36:23.299068Z","shell.execute_reply":"2023-11-15T13:36:25.025168Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"code","source":"e_p = preprocess_text_en(e)\ng_p = preprocess_text_gr(g)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:36:25.027416Z","iopub.execute_input":"2023-11-15T13:36:25.027715Z","iopub.status.idle":"2023-11-15T13:36:35.644723Z","shell.execute_reply.started":"2023-11-15T13:36:25.027691Z","shell.execute_reply":"2023-11-15T13:36:35.643859Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"g_p[:10]","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:36:35.645898Z","iopub.execute_input":"2023-11-15T13:36:35.646188Z","iopub.status.idle":"2023-11-15T13:36:35.652532Z","shell.execute_reply.started":"2023-11-15T13:36:35.646163Z","shell.execute_reply":"2023-11-15T13:36:35.651607Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"['resumption of the session',\n 'i declare resumed the session of the european parliament adjourned on friday 17 december 1999 , and i would like once again to wish you a happy new year the hope that you enjoyed a pleasant festive period .',\n \"although , as you have seen , the dreaded 'millennium bug ' failed to materialise , still the people a number of countries suffered a series of natural disasters that truly were dreadful .\",\n 'you have requested a debate on this subject the course of the next few days , during this part-session .',\n \"the meantime , i should like to observe a minute ' s silence , as a number of members have requested , on behalf of all the victims concerned , particularly those of the terrible storms , the various countries of the european union .\",\n \"please rise , then , for this minute ' s silence .\",\n \"( the house rose and observed a minute ' s silence )\",\n 'madam president , on a point of order .',\n 'you be aware from the press and television that there have been a number of bomb explosions and killings sri lanka .',\n 'one of the people assassinated very recently sri lanka mr kumar ponnambalam , who had visited the european parliament just a few months ago .']"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import GPT2Tokenizer\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\n\n# Load GPT-2 tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\n# Define a pad token and add it to the tokenizer\npad_token = tokenizer.eos_token\ntokenizer.add_tokens([pad_token])\n\n# Define a function to tokenize, convert text to indices, and pad sequences\ndef tokenize_and_pad(data_list, max_article_length=1021):\n    tokenized_data_list = []\n    for article in data_list:\n        # Tokenize and convert to indices\n        article_tokens = tokenizer.encode(article, add_special_tokens=True)\n#         highlights_tokens = tokenizer.encode(highlights, add_special_tokens=True)\n\n        # Pad sequences to specified lengths\n        padded_article_tokens = torch.tensor(article_tokens + [tokenizer.convert_tokens_to_ids(pad_token)] * (max_article_length - len(article_tokens)))\n#         padded_highlights_tokens = torch.tensor(highlights_tokens + [tokenizer.convert_tokens_to_ids(pad_token)] * (max_highlights_length - len(highlights_tokens)))\n        truncated_article_tokens = padded_article_tokens[:max_article_length]\n\n        # Append to the tokenized_data_list only if both token lists are not empty\n        if len(article_tokens) > 0:\n            tokenized_data_list.append(padded_article_tokens)\n        \n    return tokenized_data_list\n\n# Apply tokenization and padding to your datasets\n# max_article_length = 1021\n# max_highlights_length = 1024\n# tokenized_train_c = tokenize_and_pad(tokenized_train_c, max_article_length)\n# tokenized_test_data_list = tokenize_and_pad(ptest_data_list, max_article_length)\n# tokenized_val_data_list = tokenize_and_pad(pval_data_list, max_article_length)\ne_token = tokenize_and_pad(e_p, 1024)\ng_token = tokenize_and_pad(g_p, 1018)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:36:35.653695Z","iopub.execute_input":"2023-11-15T13:36:35.654121Z","iopub.status.idle":"2023-11-15T13:37:01.257472Z","shell.execute_reply.started":"2023-11-15T13:36:35.654096Z","shell.execute_reply":"2023-11-15T13:37:01.256559Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b5a81708fc44f0fbfb6ac3f1f0bb339"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1829cfb73ebd4d828d58d166a82a2524"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7f63463043c49c5af2e4eeb42b5f0ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0650fbb665cc4d8dba6ec668ad87cc6c"}},"metadata":{}}]},{"cell_type":"code","source":"g_token[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:37:01.258869Z","iopub.execute_input":"2023-11-15T13:37:01.259364Z","iopub.status.idle":"2023-11-15T13:37:01.303514Z","shell.execute_reply.started":"2023-11-15T13:37:01.259314Z","shell.execute_reply":"2023-11-15T13:37:01.302459Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"tensor([  411, 24098,   286,  ..., 50256, 50256, 50256])"},"metadata":{}}]},{"cell_type":"code","source":"e_token = torch.stack(e_token)\ng_token = torch.stack(g_token)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:37:01.304725Z","iopub.execute_input":"2023-11-15T13:37:01.305070Z","iopub.status.idle":"2023-11-15T13:37:01.452848Z","shell.execute_reply.started":"2023-11-15T13:37:01.305038Z","shell.execute_reply":"2023-11-15T13:37:01.451920Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Tokenize the word \"summarize\"\ninput_ids = tokenizer.encode(\"translate german to english\")\nprint(input_ids)","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:37:01.456782Z","iopub.execute_input":"2023-11-15T13:37:01.457186Z","iopub.status.idle":"2023-11-15T13:37:01.462999Z","shell.execute_reply.started":"2023-11-15T13:37:01.457145Z","shell.execute_reply":"2023-11-15T13:37:01.461959Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[7645, 17660, 308, 2224, 284, 46932]\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import GPT2LMHeadModel, GPT2Tokenizer\n\n# Load GPT-2 model and tokenizer\nmodel_name = \"gpt2\"\ntokenizer = GPT2Tokenizer.from_pretrained(model_name)\ngpt2_model = GPT2LMHeadModel.from_pretrained(model_name)\n\n# Define the number of prompts and embedding size\nnum_prompts = 6  # \"summarize the following text\"\nembedding_size = 768\n\n# Define a specific sentence\nsentence = \"translate german to english\"\n\n# Tokenize the sentence\ninput_ids = tokenizer.encode(sentence, return_tensors='pt')\n\n# Get the embeddings for the input_ids from the GPT-2 model\ngpt2_embeddings = gpt2_model.transformer.wte(input_ids)\n\n# Create an embedding layer for soft prompts and initialize with the sentence embeddings\nsoft_prompt_embeddings = nn.Embedding(num_prompts, embedding_size)\nsoft_prompt_embeddings.weight.data.copy_(gpt2_embeddings.squeeze(0))\n\n# Concatenate soft prompt embeddings at the beginning of the input sequence\nclass GPT2WithPromptTuning(nn.Module):\n    def __init__(self, gpt2_model, soft_prompt_embeddings):\n        super(GPT2WithPromptTuning, self).__init__()\n        self.gpt2_model = gpt2_model\n        self.soft_prompt_embeddings = soft_prompt_embeddings\n    \n    def forward(self, input_ids, soft_prompt_ids):\n        # Get the embeddings for the input_ids from the GPT-2 model\n        gpt2_embeddings = self.gpt2_model.transformer.wte(input_ids)\n        # Get the embeddings for the soft prompts\n        soft_prompt_embeds = self.soft_prompt_embeddings(soft_prompt_ids)\n        \n        # Concatenate the embeddings\n        embeddings = torch.cat([soft_prompt_embeds, gpt2_embeddings], dim=0)\n        \n        # Pass the concatenated embeddings through the GPT-2 model\n        outputs = self.gpt2_model(inputs_embeds=embeddings)\n        \n        return outputs\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:37:01.464186Z","iopub.execute_input":"2023-11-15T13:37:01.464470Z","iopub.status.idle":"2023-11-15T13:37:06.474496Z","shell.execute_reply.started":"2023-11-15T13:37:01.464447Z","shell.execute_reply":"2023-11-15T13:37:06.473752Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca6b70be68e545a0b644194c3fc6e5a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78a8640475c64eca9e35503cbb0fa8e5"}},"metadata":{}}]},{"cell_type":"code","source":"# Initialize the model\nmodel = GPT2WithPromptTuning(gpt2_model, soft_prompt_embeddings)\n\n# Freeze GPT-2 model weights\nfor param in model.gpt2_model.parameters():\n    param.requires_grad = False\n\n# Define hyperparameters\nbatch_size = 8\nepochs = 5\nlearning_rate = 2e-5\ngradient_clip_value = 1.0\n\n# Define optimizer and criterion\noptimizer = torch.optim.AdamW(model.soft_prompt_embeddings.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\n\n# Example data (replace with your own dataset)\n# input_ids = torch.randint(0, 100, (1023,))  # Example input sequence\nsoft_prompt_ids = torch.tensor([0, 1, 2, 3 ,4 ,5])\n# target_ids = torch.randint(0, 100, (1024,))  # Example input sequence\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:37:06.475501Z","iopub.execute_input":"2023-11-15T13:37:06.475903Z","iopub.status.idle":"2023-11-15T13:37:06.483331Z","shell.execute_reply.started":"2023-11-15T13:37:06.475878Z","shell.execute_reply":"2023-11-15T13:37:06.482416Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm\nimport nltk\nfrom nltk.translate.bleu_score import sentence_bleu\n# from nltk.translate.meteor_score import meteor_score\n# from nltk.translate.rouge import Rouge\n\n# Download necessary resources for nltk (run this once)\nnltk.download(\"wordnet\")\nnltk.download(\"averaged_perceptron_tagger\")\nnltk.download(\"universal_tagset\")\nnltk.download(\"punkt\")\nnltk.download(\"stopwords\")","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:37:06.484464Z","iopub.execute_input":"2023-11-15T13:37:06.484797Z","iopub.status.idle":"2023-11-15T13:37:06.526490Z","shell.execute_reply.started":"2023-11-15T13:37:06.484762Z","shell.execute_reply":"2023-11-15T13:37:06.525583Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n[nltk_data] Downloading package universal_tagset to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package universal_tagset is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm\nimport wandb\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Move model to GPU\nmodel.to(device)\n\n# Define device (assuming you have a CUDA-enabled GPU)\n\n# Move optimizer and criterion to GPU\noptimizer = torch.optim.AdamW(model.soft_prompt_embeddings.parameters(), lr=learning_rate)\ncriterion = nn.CrossEntropyLoss(ignore_index=-100).to(device)\n\n# Initialize Weights and Biases\nwandb.init(project='SoftPrompt on GPT-2 for Translation', name='1', config={'learning_rate': learning_rate})\n\n# Training loop\nfor epoch in range(epochs):\n    # Create a tqdm progress bar for the training data\n    data_iterator = tqdm(zip(g_token, e_token), desc=f'Epoch {epoch + 1}', total=len(g_token))\n    \n    for input_ids, target_ids in data_iterator:\n        optimizer.zero_grad()\n\n        # Move input and target tensors to GPU\n        input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n        \n        # Assuming you have a soft_prompt_ids for each training instance\n        # If not, you might need to modify this part accordingly\n        outputs = model(input_ids, soft_prompt_ids.to(device))\n        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n\n        loss = criterion(logits, target_ids)\n        loss.backward()\n\n        # Gradient clipping to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_value)\n\n        optimizer.step()\n\n        # Update the progress bar description with the current loss\n        data_iterator.set_postfix(loss=loss.item())\n\n# #     # Validation loop\n#     model.eval()\n#     val_losses = []\n#     with torch.no_grad():\n#         for input_ids_val, target_ids_val in zip(input_ids_val, target_ids_val):\n#             input_ids_val, target_ids_val = input_ids_val.to(device), target_ids_val.to(device)\n#             outputs_val = model(input_ids_val, soft_prompt_ids.to(device))\n#             logits_val = outputs_val.logits if hasattr(outputs_val, \"logits\") else outputs_val.last_hidden_state\n#             loss_val = criterion(logits_val, target_ids_val)\n#             val_losses.append(loss_val.item())\n#             # Convert tensor predictions and references to lists\n#             predictions = logits_val.argmax(dim=-1).squeeze(0).tolist()\n#             references = target_ids_val.squeeze(0).tolist()\n\n#             # BLEU Score\n#             bleu_score = sentence_bleu([references], predictions)\n#             print(f\"BLEU Score: {bleu_score}\")\n#     avg_val_loss = sum(val_losses) / len(val_losses)\n    wandb.log({\"epoch\": epoch + 1, \"train_loss\": loss.item()})\n\n    # Set the model back to training mode\n#     model.train()\n\n# Close the tqdm progress bar\ndata_iterator.close()\n","metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:37:06.527561Z","iopub.execute_input":"2023-11-15T13:37:06.527801Z","iopub.status.idle":"2023-11-15T14:46:59.504369Z","shell.execute_reply.started":"2023-11-15T13:37:06.527780Z","shell.execute_reply":"2023-11-15T14:46:59.502704Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.0 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.12"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231115_133722-xhsrjlu9</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/anlp_sarcasmdetection/SoftPrompt%20on%20GPT-2%20for%20Translation/runs/xhsrjlu9' target=\"_blank\">1</a></strong> to <a href='https://wandb.ai/anlp_sarcasmdetection/SoftPrompt%20on%20GPT-2%20for%20Translation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/anlp_sarcasmdetection/SoftPrompt%20on%20GPT-2%20for%20Translation' target=\"_blank\">https://wandb.ai/anlp_sarcasmdetection/SoftPrompt%20on%20GPT-2%20for%20Translation</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/anlp_sarcasmdetection/SoftPrompt%20on%20GPT-2%20for%20Translation/runs/xhsrjlu9' target=\"_blank\">https://wandb.ai/anlp_sarcasmdetection/SoftPrompt%20on%20GPT-2%20for%20Translation/runs/xhsrjlu9</a>"},"metadata":{}},{"name":"stderr","text":"Epoch 1: 100%|██████████| 9968/9968 [39:29<00:00,  4.21it/s, loss=0.401]\nEpoch 2:  75%|███████▍  | 7472/9968 [29:34<09:52,  4.21it/s, loss=0.284]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m input_ids, target_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device), target_ids\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Assuming you have a soft_prompt_ids for each training instance\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If not, you might need to modify this part accordingly\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoft_prompt_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, target_ids)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[15], line 44\u001b[0m, in \u001b[0;36mGPT2WithPromptTuning.forward\u001b[0;34m(self, input_ids, soft_prompt_ids)\u001b[0m\n\u001b[1;32m     41\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([soft_prompt_embeds, gpt2_embeddings], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Pass the concatenated embeddings through the GPT-2 model\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt2_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:201\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    198\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(causal_mask, attn_weights\u001b[38;5;241m.\u001b[39mto(attn_weights\u001b[38;5;241m.\u001b[39mdtype), mask_value)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}