# -*- coding: utf-8 -*-
"""softprompt-gpt2-translation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PfNVUycbxldKVcV83thoKdCqkM_8fwPx
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import wandb
from nltk.translate.bleu_score import sentence_bleu
from tqdm import tqdm
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch.nn as nn
from torch.nn.utils.rnn import pad_sequence
import torch
from transformers import GPT2Tokenizer
import string
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
import nltk
import numpy as np  # linear algebra
import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All"
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session

# import tarfile

# # Replace 'your_file.tgz' with the actual file name
# file_path = '/kaggle/input/europarl-dataset/de-en.tgz'

# # Extract the contents of the tarball
# with tarfile.open(file_path, 'r:gz') as tar:
#     tar.extractall(path='/kaggle/working/')

file_path = '/kaggle/input/europarl-dataset/europarl-v7.de-en.de'

# Read the contents of the file into a list
with open(file_path, 'r', encoding='utf-8') as file:
    eng = file.readlines()

# Display the first few lines of the list
print(eng[:5])

len(eng)

file_path = '/kaggle/input/europarl-dataset/europarl-v7.de-en.en'

# Read the contents of the file into a list
with open(file_path, 'r', encoding='utf-8') as file:
    ger = file.readlines()

# Display the first few lines of the list
print(ger[:5])

len(ger)

e = eng[:10000]
g = ger[:10000]


# Download NLTK resources (if not already downloaded)
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Function for text preprocessing


def preprocess_text_en(data):
    # Convert to lowercase
    data2 = []
    for text in data:
        text = text.lower()

        # Remove punctuation and digits
#         text = text.translate(str.maketrans('', '', string.punctuation + string.digits))

        # Tokenization
        tokens = word_tokenize(text)

        # Remove stop words
        stop_words = set(stopwords.words('english'))
        tokens = [word for word in tokens if word not in stop_words]

        # Lemmatization
    #     lemmatizer = WordNetLemmatizer()
    #     tokens = [lemmatizer.lemmatize(word) for word in tokens]

        # Join the tokens back into a string
        preprocessed_text = ' '.join(tokens)
        data2.append(preprocessed_text)

    return data2


def preprocess_text_gr(data):
    # Convert to lowercase
    data2 = []
    for text in data:
        text = text.lower()

        # Remove punctuation and digits
#         text = text.translate(str.maketrans('', '', string.punctuation + string.digits))

        # Tokenization
        tokens = word_tokenize(text)

        # Remove stop words
        stop_words = set(stopwords.words('german'))
        tokens = [word for word in tokens if word not in stop_words]

        # Lemmatization
    #     lemmatizer = WordNetLemmatizer()
    #     tokens = [lemmatizer.lemmatize(word) for word in tokens]

        # Join the tokens back into a string
        preprocessed_text = ' '.join(tokens)
        data2.append(preprocessed_text)

    return data2


e_p = preprocess_text_en(e)
g_p = preprocess_text_gr(g)

g_p[:10]


# Load GPT-2 tokenizer
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")

# Define a pad token and add it to the tokenizer
pad_token = tokenizer.eos_token
tokenizer.add_tokens([pad_token])

# Define a function to tokenize, convert text to indices, and pad sequences


def tokenize_and_pad(data_list, max_article_length=1021):
    tokenized_data_list = []
    for article in data_list:
        # Tokenize and convert to indices
        article_tokens = tokenizer.encode(article, add_special_tokens=True)
#         highlights_tokens = tokenizer.encode(highlights, add_special_tokens=True)

        # Pad sequences to specified lengths
        padded_article_tokens = torch.tensor(
            article_tokens + [tokenizer.convert_tokens_to_ids(pad_token)] * (max_article_length - len(article_tokens)))
#         padded_highlights_tokens = torch.tensor(highlights_tokens + [tokenizer.convert_tokens_to_ids(pad_token)] * (max_highlights_length - len(highlights_tokens)))
        truncated_article_tokens = padded_article_tokens[:max_article_length]

        # Append to the tokenized_data_list only if both token lists are not empty
        if len(article_tokens) > 0:
            tokenized_data_list.append(padded_article_tokens)

    return tokenized_data_list


# Apply tokenization and padding to your datasets
# max_article_length = 1021
# max_highlights_length = 1024
# tokenized_train_c = tokenize_and_pad(tokenized_train_c, max_article_length)
# tokenized_test_data_list = tokenize_and_pad(ptest_data_list, max_article_length)
# tokenized_val_data_list = tokenize_and_pad(pval_data_list, max_article_length)
e_token = tokenize_and_pad(e_p, 1024)
g_token = tokenize_and_pad(g_p, 1018)

g_token[0]

e_token = torch.stack(e_token)
g_token = torch.stack(g_token)

# Tokenize the word "summarize"
input_ids = tokenizer.encode("translate german to english")
print(input_ids)


# Load GPT-2 model and tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
gpt2_model = GPT2LMHeadModel.from_pretrained(model_name)

# Define the number of prompts and embedding size
num_prompts = 6  # "summarize the following text"
embedding_size = 768

# Define a specific sentence
sentence = "translate german to english"

# Tokenize the sentence
input_ids = tokenizer.encode(sentence, return_tensors='pt')

# Get the embeddings for the input_ids from the GPT-2 model
gpt2_embeddings = gpt2_model.transformer.wte(input_ids)

# Create an embedding layer for soft prompts and initialize with the sentence embeddings
soft_prompt_embeddings = nn.Embedding(num_prompts, embedding_size)
soft_prompt_embeddings.weight.data.copy_(gpt2_embeddings.squeeze(0))

# Concatenate soft prompt embeddings at the beginning of the input sequence


class GPT2WithPromptTuning(nn.Module):
    def __init__(self, gpt2_model, soft_prompt_embeddings):
        super(GPT2WithPromptTuning, self).__init__()
        self.gpt2_model = gpt2_model
        self.soft_prompt_embeddings = soft_prompt_embeddings

    def forward(self, input_ids, soft_prompt_ids):
        # Get the embeddings for the input_ids from the GPT-2 model
        gpt2_embeddings = self.gpt2_model.transformer.wte(input_ids)
        # Get the embeddings for the soft prompts
        soft_prompt_embeds = self.soft_prompt_embeddings(soft_prompt_ids)

        # Concatenate the embeddings
        embeddings = torch.cat([soft_prompt_embeds, gpt2_embeddings], dim=0)

        # Pass the concatenated embeddings through the GPT-2 model
        outputs = self.gpt2_model(inputs_embeds=embeddings)

        return outputs


# Initialize the model
model = GPT2WithPromptTuning(gpt2_model, soft_prompt_embeddings)

# Freeze GPT-2 model weights
for param in model.gpt2_model.parameters():
    param.requires_grad = False

# Define hyperparameters
batch_size = 8
epochs = 5
learning_rate = 2e-5
gradient_clip_value = 1.0

# Define optimizer and criterion
optimizer = torch.optim.AdamW(
    model.soft_prompt_embeddings.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss(ignore_index=-100)

# Example data (replace with your own dataset)
# input_ids = torch.randint(0, 100, (1023,))  # Example input sequence
soft_prompt_ids = torch.tensor([0, 1, 2, 3, 4, 5])
# target_ids = torch.randint(0, 100, (1024,))  # Example input sequence

# from nltk.translate.meteor_score import meteor_score
# from nltk.translate.rouge import Rouge

# Download necessary resources for nltk (run this once)
nltk.download("wordnet")
nltk.download("averaged_perceptron_tagger")
nltk.download("universal_tagset")
nltk.download("punkt")
nltk.download("stopwords")


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Move model to GPU
model.to(device)

# Define device (assuming you have a CUDA-enabled GPU)

# Move optimizer and criterion to GPU
optimizer = torch.optim.AdamW(
    model.soft_prompt_embeddings.parameters(), lr=learning_rate)
criterion = nn.CrossEntropyLoss(ignore_index=-100).to(device)

# Initialize Weights and Biases
wandb.init(project='SoftPrompt on GPT-2 for Translation',
           name='1', config={'learning_rate': learning_rate})

# Training loop
for epoch in range(epochs):
    # Create a tqdm progress bar for the training data
    data_iterator = tqdm(zip(g_token, e_token),
                         desc=f'Epoch {epoch + 1}', total=len(g_token))

    for input_ids, target_ids in data_iterator:
        optimizer.zero_grad()

        # Move input and target tensors to GPU
        input_ids, target_ids = input_ids.to(device), target_ids.to(device)

        # Assuming you have a soft_prompt_ids for each training instance
        # If not, you might need to modify this part accordingly
        outputs = model(input_ids, soft_prompt_ids.to(device))
        logits = outputs.logits if hasattr(
            outputs, "logits") else outputs.last_hidden_state

        loss = criterion(logits, target_ids)
        loss.backward()

        # Gradient clipping to prevent exploding gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_value)

        optimizer.step()

        # Update the progress bar description with the current loss
        data_iterator.set_postfix(loss=loss.item())

# #     # Validation loop
#     model.eval()
#     val_losses = []
#     with torch.no_grad():
#         for input_ids_val, target_ids_val in zip(input_ids_val, target_ids_val):
#             input_ids_val, target_ids_val = input_ids_val.to(device), target_ids_val.to(device)
#             outputs_val = model(input_ids_val, soft_prompt_ids.to(device))
#             logits_val = outputs_val.logits if hasattr(outputs_val, "logits") else outputs_val.last_hidden_state
#             loss_val = criterion(logits_val, target_ids_val)
#             val_losses.append(loss_val.item())
#             # Convert tensor predictions and references to lists
#             predictions = logits_val.argmax(dim=-1).squeeze(0).tolist()
#             references = target_ids_val.squeeze(0).tolist()

#             # BLEU Score
#             bleu_score = sentence_bleu([references], predictions)
#             print(f"BLEU Score: {bleu_score}")
#     avg_val_loss = sum(val_losses) / len(val_losses)
    wandb.log({"epoch": epoch + 1, "train_loss": loss.item()})

    # Set the model back to training mode
#     model.train()

# Close the tqdm progress bar
data_iterator.close()

#  translate german to eng : trainbale ( layer )
#  german (input )

#  english
