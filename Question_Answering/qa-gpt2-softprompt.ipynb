{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-15T13:33:34.853966Z","iopub.status.busy":"2023-11-15T13:33:34.853221Z","iopub.status.idle":"2023-11-15T13:33:35.245095Z","shell.execute_reply":"2023-11-15T13:33:35.244056Z","shell.execute_reply.started":"2023-11-15T13:33:34.853925Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/squad-20/train-v2.0.json\n","/kaggle/input/squad-20/dev-v2.0.json\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:33:35.247304Z","iopub.status.busy":"2023-11-15T13:33:35.246769Z","iopub.status.idle":"2023-11-15T13:33:37.269902Z","shell.execute_reply":"2023-11-15T13:33:37.268815Z","shell.execute_reply.started":"2023-11-15T13:33:35.247274Z"},"trusted":true},"outputs":[],"source":["import json\n","\n","def extract_squad_data(json_file_path):\n","    with open(json_file_path, 'r', encoding='utf-8') as file:\n","        squad_data = json.load(file)\n","    \n","    data = squad_data['data']\n","    extracted_data = []\n","\n","    for entry in data:\n","        paragraphs = entry['paragraphs']\n","        for paragraph in paragraphs:\n","            context = paragraph['context']\n","            qas = paragraph['qas']\n","            for qa in qas:\n","                question = qa['question']\n","                is_impossible = qa['is_impossible']\n","                \n","                if not is_impossible:\n","                    answers = [answer['text'] for answer in qa['answers']]\n","                else:\n","                    answers = []\n","\n","                extracted_data.append({\n","                    'context': context,\n","                    'question': question,\n","                    'answers': answers,\n","                    'is_impossible': is_impossible\n","                })\n","\n","    return extracted_data\n","\n","# Example usage:\n","json_file_path = '/kaggle/input/squad-20/train-v2.0.json'\n","train_data = extract_squad_data(json_file_path)\n","val_data = extract_squad_data('/kaggle/input/squad-20/dev-v2.0.json')\n","# You can now use the processed_data for further preprocessing or analysis.\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:33:37.272916Z","iopub.status.busy":"2023-11-15T13:33:37.272467Z","iopub.status.idle":"2023-11-15T13:33:37.301924Z","shell.execute_reply":"2023-11-15T13:33:37.300868Z","shell.execute_reply.started":"2023-11-15T13:33:37.272876Z"},"trusted":true},"outputs":[],"source":["val_data = [item for item in val_data if not item['is_impossible']]\n","train_data = [item for item in train_data if not item['is_impossible']]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:33:37.304549Z","iopub.status.busy":"2023-11-15T13:33:37.304203Z","iopub.status.idle":"2023-11-15T13:33:37.314430Z","shell.execute_reply":"2023-11-15T13:33:37.313370Z","shell.execute_reply.started":"2023-11-15T13:33:37.304519Z"},"trusted":true},"outputs":[{"data":{"text/plain":["{'context': 'If a problem X is in C and hard for C, then X is said to be complete for C. This means that X is the hardest problem in C. (Since many problems could be equally hard, one might say that X is one of the hardest problems in C.) Thus the class of NP-complete problems contains the most difficult problems in NP, in the sense that they are the ones most likely not to be in P. Because the problem P = NP is not solved, being able to reduce a known NP-complete problem, Π2, to another problem, Π1, would indicate that there is no known polynomial-time solution for Π1. This is because a polynomial-time solution to Π1 would yield a polynomial-time solution to Π2. Similarly, because all NP problems can be reduced to the set, finding an NP-complete problem that can be solved in polynomial time would mean that P = NP.',\n"," 'question': 'If P = NP is unsolved, and reduction is applied to a known NP-complete problem vis a vis Π2 to  Π1, what conclusion can be drawn for Π1?',\n"," 'answers': ['there is no known polynomial-time solution',\n","  'no known polynomial-time solution',\n","  'there is no known polynomial-time solution'],\n"," 'is_impossible': False}"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["val_data[231]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:33:37.316227Z","iopub.status.busy":"2023-11-15T13:33:37.315759Z","iopub.status.idle":"2023-11-15T13:33:37.372499Z","shell.execute_reply":"2023-11-15T13:33:37.371836Z","shell.execute_reply.started":"2023-11-15T13:33:37.316198Z"},"trusted":true},"outputs":[],"source":["train_content = []\n","train_q = []\n","train_a = []\n","\n","for block in train_data:\n","    train_content.append(block['context'])\n","    train_q.append(block['question'])\n","    train_a.append(block['answers'])\n","    \n","    \n","val_content = []\n","val_q = []\n","val_a = []\n","\n","for block in val_data:\n","    val_content.append(block['context'])\n","    val_q.append(block['question'])\n","    val_a.append(block['answers'])"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:33:37.373674Z","iopub.status.busy":"2023-11-15T13:33:37.373385Z","iopub.status.idle":"2023-11-15T13:33:37.378640Z","shell.execute_reply":"2023-11-15T13:33:37.377780Z","shell.execute_reply.started":"2023-11-15T13:33:37.373649Z"},"trusted":true},"outputs":[],"source":["# onet_content = train_content[:1000]\n","# onet_q = train_q[:1000]\n","# onet_a = train_a[:1000]\n","# onev_content = val_content[:100]\n","# onev_q = val_q[:100]\n","# onev_a = val_a[:100]"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:33:37.380048Z","iopub.status.busy":"2023-11-15T13:33:37.379709Z","iopub.status.idle":"2023-11-15T13:33:38.672822Z","shell.execute_reply":"2023-11-15T13:33:38.671716Z","shell.execute_reply.started":"2023-11-15T13:33:37.380017Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]},{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","import string\n","\n","# Download NLTK resources (if not already downloaded)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Function for text preprocessing\n","def preprocess_text(data):\n","    # Convert to lowercase\n","    data2 = []\n","    for text in data:\n","        text = text.lower()\n","\n","        # Remove punctuation and digits\n","#         text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n","\n","        # Tokenization\n","        tokens = word_tokenize(text)\n","\n","        # Remove stop words\n","        stop_words = set(stopwords.words('english'))\n","        tokens = [word for word in tokens if word not in stop_words]\n","\n","        # Lemmatization\n","    #     lemmatizer = WordNetLemmatizer()\n","    #     tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","        # Join the tokens back into a string\n","        preprocessed_text = ' '.join(tokens)\n","        data2.append(preprocessed_text)\n","\n","    return data2\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:33:38.674653Z","iopub.status.busy":"2023-11-15T13:33:38.674240Z","iopub.status.idle":"2023-11-15T13:33:38.679285Z","shell.execute_reply":"2023-11-15T13:33:38.678246Z","shell.execute_reply.started":"2023-11-15T13:33:38.674614Z"},"trusted":true},"outputs":[],"source":["# onet_q[0][0]"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:33:38.680679Z","iopub.status.busy":"2023-11-15T13:33:38.680360Z","iopub.status.idle":"2023-11-15T13:33:38.690191Z","shell.execute_reply":"2023-11-15T13:33:38.689239Z","shell.execute_reply.started":"2023-11-15T13:33:38.680646Z"},"trusted":true},"outputs":[{"data":{"text/plain":["86821"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["len(train_content)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:33:38.694804Z","iopub.status.busy":"2023-11-15T13:33:38.694406Z","iopub.status.idle":"2023-11-15T13:33:38.725714Z","shell.execute_reply":"2023-11-15T13:33:38.724850Z","shell.execute_reply.started":"2023-11-15T13:33:38.694750Z"},"trusted":true},"outputs":[],"source":["train_a = [item for sublist in train_a for item in sublist]\n","val_a = [item for sublist in val_a for item in sublist]"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:33:38.727340Z","iopub.status.busy":"2023-11-15T13:33:38.727012Z","iopub.status.idle":"2023-11-15T13:37:29.509422Z","shell.execute_reply":"2023-11-15T13:37:29.508531Z","shell.execute_reply.started":"2023-11-15T13:33:38.727309Z"},"trusted":true},"outputs":[],"source":["onet_content = preprocess_text(train_content)\n","onet_q = preprocess_text(train_q)\n","onet_a = preprocess_text(train_a)\n","\n","onev_content = preprocess_text(val_content)\n","onev_q = preprocess_text(val_q)\n","onev_a = preprocess_text(val_a)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:37:29.510779Z","iopub.status.busy":"2023-11-15T13:37:29.510531Z","iopub.status.idle":"2023-11-15T13:37:29.517158Z","shell.execute_reply":"2023-11-15T13:37:29.516067Z","shell.execute_reply.started":"2023-11-15T13:37:29.510756Z"},"trusted":true},"outputs":[{"data":{"text/plain":["86821"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["len(onet_a)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:37:29.518578Z","iopub.status.busy":"2023-11-15T13:37:29.518298Z","iopub.status.idle":"2023-11-15T13:37:29.528629Z","shell.execute_reply":"2023-11-15T13:37:29.527828Z","shell.execute_reply.started":"2023-11-15T13:37:29.518551Z"},"trusted":true},"outputs":[],"source":["\n","# from transformers import GPT2Tokenizer\n","\n","# # Load GPT-2 tokenizer\n","# tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","# # Define a function to tokenize, convert text to indices, and truncate to the first 1021 tokens\n","# def tokenize_and_convert_to_indices(data_list, max_length):\n","#     tokenized_data_list = []\n","#     for article in data_list:\n","#         # Tokenize and convert to indices\n","#         article_tokens = tokenizer.encode(article, add_special_tokens=True)\n","# #         highlights_tokens = tokenizer.encode(highlights, add_special_tokens=True)\n","\n","#         # Truncate to the first 1021 tokens\n","#         article_tokens = article_tokens[:max_length]\n","# #         highlights_tokens = highlights_tokens[:max_length+3]\n","\n","#         # Append to the tokenized_data_list\n","#         tokenized_data_list.append(article_tokens)\n","\n","#     return tokenized_data_list\n","\n","# # Apply tokenization and truncation to your datasets\n","# max_length = 1021\n","# tokenized_train_c = tokenize_and_convert_to_indices(onet_content, 952)\n","# tokenized_train_q = tokenize_and_convert_to_indices(onet_q, 65)\n","# tokenized_train_a = tokenize_and_convert_to_indices(onet_a, 1024)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:37:29.530202Z","iopub.status.busy":"2023-11-15T13:37:29.529845Z","iopub.status.idle":"2023-11-15T13:37:29.541567Z","shell.execute_reply":"2023-11-15T13:37:29.540829Z","shell.execute_reply.started":"2023-11-15T13:37:29.530170Z"},"trusted":true},"outputs":[],"source":["# tokenized_val_c = tokenize_and_convert_to_indices(onev_content, 952)\n","# tokenized_val_q = tokenize_and_convert_to_indices(onev_q, 65)\n","# tokenized_val_a = tokenize_and_convert_to_indices(onev_a, 1024)"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:37:29.543378Z","iopub.status.busy":"2023-11-15T13:37:29.542864Z","iopub.status.idle":"2023-11-15T13:37:34.588755Z","shell.execute_reply":"2023-11-15T13:37:34.587779Z","shell.execute_reply.started":"2023-11-15T13:37:29.543345Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"732d8619d8324894a7ffc9abc8ed04e8","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"02328a713af54c6691fdb54a6d05e040","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f8fc7ac5c8dc4fe9b966f0b2b893ff56","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29a572716cbd477692a5fb144859578a","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[22866, 1577, 3280, 329, 262, 1708, 1808]\n"]}],"source":["from transformers import GPT2Tokenizer\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","\n","# Load GPT-2 tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","# context give answers for the following question\n","# Tokenize the word \"summarize\"\n","tokens = tokenizer.encode(\"context give answer for the following question\")\n","print(tokens)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:37:34.590707Z","iopub.status.busy":"2023-11-15T13:37:34.590293Z","iopub.status.idle":"2023-11-15T13:37:34.594967Z","shell.execute_reply":"2023-11-15T13:37:34.593698Z","shell.execute_reply.started":"2023-11-15T13:37:34.590680Z"},"trusted":true},"outputs":[],"source":["# context  : 952\n","# prompt   : 7\n","# question : 65"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:37:34.596614Z","iopub.status.busy":"2023-11-15T13:37:34.596241Z","iopub.status.idle":"2023-11-15T13:41:07.005670Z","shell.execute_reply":"2023-11-15T13:41:07.004841Z","shell.execute_reply.started":"2023-11-15T13:37:34.596576Z"},"trusted":true},"outputs":[],"source":["from transformers import GPT2Tokenizer\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","\n","# Load GPT-2 tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","# Define a pad token and add it to the tokenizer\n","pad_token = tokenizer.eos_token\n","tokenizer.add_tokens([pad_token])\n","\n","# Define a function to tokenize, convert text to indices, and pad sequences\n","def tokenize_and_pad(data_list, max_article_length=1021):\n","    tokenized_data_list = []\n","    for article in data_list:\n","        # Tokenize and convert to indices\n","        article_tokens = tokenizer.encode(article, add_special_tokens=True)\n","#         highlights_tokens = tokenizer.encode(highlights, add_special_tokens=True)\n","\n","        # Pad sequences to specified lengths\n","        padded_article_tokens = torch.tensor(article_tokens + [tokenizer.convert_tokens_to_ids(pad_token)] * (max_article_length - len(article_tokens)))\n","#         padded_highlights_tokens = torch.tensor(highlights_tokens + [tokenizer.convert_tokens_to_ids(pad_token)] * (max_highlights_length - len(highlights_tokens)))\n","        truncated_article_tokens = padded_article_tokens[:max_article_length]\n","\n","        # Append to the tokenized_data_list only if both token lists are not empty\n","        if len(article_tokens) > 0:\n","            tokenized_data_list.append(padded_article_tokens)\n","        \n","    return tokenized_data_list\n","\n","# Apply tokenization and padding to your datasets\n","# max_article_length = 1021\n","# max_highlights_length = 1024\n","# tokenized_train_c = tokenize_and_pad(tokenized_train_c, max_article_length)\n","# tokenized_test_data_list = tokenize_and_pad(ptest_data_list, max_article_length)\n","# tokenized_val_data_list = tokenize_and_pad(pval_data_list, max_article_length)\n","tokenized_train_c = tokenize_and_pad(onet_content, 952)\n","tokenized_train_q = tokenize_and_pad(onet_q, 65)\n","tokenized_train_a = tokenize_and_pad(onet_a, 1024)\n","tokenized_val_c = tokenize_and_pad(onev_content, 952)\n","tokenized_val_q = tokenize_and_pad(onev_q, 65)\n","tokenized_val_a = tokenize_and_pad(onev_a, 1024)\n"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:41:07.007421Z","iopub.status.busy":"2023-11-15T13:41:07.007144Z","iopub.status.idle":"2023-11-15T13:41:07.305639Z","shell.execute_reply":"2023-11-15T13:41:07.304803Z","shell.execute_reply.started":"2023-11-15T13:41:07.007395Z"},"trusted":true},"outputs":[],"source":["tokenized_trainqqq = []\n","\n","for padded_article_tokens in tokenized_train_q:\n","    # Truncate article tokens if greater than max_article_length\n","    truncated_article_tokens = padded_article_tokens[:65]\n","    tokenized_trainqqq.append(truncated_article_tokens)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:41:07.307191Z","iopub.status.busy":"2023-11-15T13:41:07.306855Z","iopub.status.idle":"2023-11-15T13:41:08.616143Z","shell.execute_reply":"2023-11-15T13:41:08.615361Z","shell.execute_reply.started":"2023-11-15T13:41:07.307163Z"},"trusted":true},"outputs":[],"source":["# Convert the lists to PyTorch tensors\n","c_train = torch.stack(tokenized_train_c)\n","q_train = torch.stack(tokenized_trainqqq)\n","a_train = torch.stack(tokenized_train_a)\n","c_val = torch.stack(tokenized_val_c)\n","q_val = torch.stack(tokenized_val_q)\n","q_val = torch.stack(tokenized_val_a)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:41:08.617410Z","iopub.status.busy":"2023-11-15T13:41:08.617138Z","iopub.status.idle":"2023-11-15T13:41:08.629046Z","shell.execute_reply":"2023-11-15T13:41:08.628001Z","shell.execute_reply.started":"2023-11-15T13:41:08.617385Z"},"trusted":true},"outputs":[{"data":{"text/plain":["65"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["len(q_train[19339])"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:41:08.630571Z","iopub.status.busy":"2023-11-15T13:41:08.630248Z","iopub.status.idle":"2023-11-15T13:41:14.582575Z","shell.execute_reply":"2023-11-15T13:41:14.581827Z","shell.execute_reply.started":"2023-11-15T13:41:08.630540Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0eb962fecc634a87af3f7bd1aff1587d","version_major":2,"version_minor":0},"text/plain":["Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b44ecf11c57f4aaaa6548a224d91c711","version_major":2,"version_minor":0},"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import torch\n","import torch.nn as nn\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, AdamW\n","\n","# Load GPT-2 model and tokenizer\n","model_name = \"gpt2\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","gpt2_model = GPT2LMHeadModel.from_pretrained(model_name)\n","\n","# Define the number of prompts and embedding size\n","num_prompts = 7  # \"summarize the following text\"\n","embedding_size = 768\n","\n","# Define a specific sentence\n","sentence = \"context give answer for the following question\"\n","\n","# Tokenize the sentence\n","input_ids = tokenizer.encode(sentence, return_tensors='pt')\n","\n","# Get the embeddings for the input_ids from the GPT-2 model\n","gpt2_embeddings = gpt2_model.transformer.wte(input_ids)\n","\n","# Create an embedding layer for soft prompts and initialize with the sentence embeddings\n","soft_prompt_embeddings = nn.Embedding(num_prompts, embedding_size)\n","soft_prompt_embeddings.weight.data.copy_(gpt2_embeddings.squeeze(0))\n","\n","# Concatenate soft prompt embeddings at the beginning of the input sequence\n","class GPT2WithPromptTuning(nn.Module):\n","    def __init__(self, gpt2_model, soft_prompt_embeddings):\n","        super(GPT2WithPromptTuning, self).__init__()\n","        self.gpt2_model = gpt2_model\n","        self.soft_prompt_embeddings = soft_prompt_embeddings\n","    \n","    def forward(self, input_c, input_q, soft_prompt_c, soft_prompt_q):\n","        # Get the embeddings for the input_ids from the GPT-2 model\n","#         gpt2_outputs = self.gpt2_model(input_ids)\n","#         gpt2_embeddings = gpt2_outputs['last_hidden_state']\n","        c_embeddings = self.gpt2_model.transformer.wte(input_c)\n","        q_embeddings = self.gpt2_model.transformer.wte(input_q)\n","\n","        # Get the embeddings for the soft prompts\n","        soft_prompt_c = self.soft_prompt_embeddings(soft_prompt_c)\n","        soft_prompt_q = self.soft_prompt_embeddings(soft_prompt_q)\n","\n","        # Concatenate the embeddings\n","        embeddings = torch.cat([soft_prompt_c, c_embeddings ,soft_prompt_q, q_embeddings], dim=0)\n","        \n","        # Pass the concatenated embeddings through the GPT-2 model\n","        outputs = self.gpt2_model(inputs_embeds=embeddings)\n","        \n","        return outputs "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:41:14.583977Z","iopub.status.busy":"2023-11-15T13:41:14.583576Z","iopub.status.idle":"2023-11-15T13:41:14.591946Z","shell.execute_reply":"2023-11-15T13:41:14.590965Z","shell.execute_reply.started":"2023-11-15T13:41:14.583951Z"},"trusted":true},"outputs":[],"source":["# Initialize the model\n","model = GPT2WithPromptTuning(gpt2_model, soft_prompt_embeddings)\n","\n","# Freeze GPT-2 model weights\n","for param in model.gpt2_model.parameters():\n","    param.requires_grad = False\n","\n","# Define hyperparameters\n","batch_size = 8\n","epochs = 5\n","learning_rate = 2e-5\n","gradient_clip_value = 1.0\n","\n","# Define optimizer and criterion\n","optimizer = torch.optim.AdamW(model.soft_prompt_embeddings.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss(ignore_index=-100)\n","\n","# Example data (replace with your own dataset)\n","# input_ids = torch.randint(0, 100, (1023,))  # Example input sequence\n","soft_prompt_c = torch.tensor([0])\n","soft_prompt_q = torch.tensor([1, 2, 3, 4, 5, 6])\n","# target_ids = torch.randint(0, 100, (1024,))  # Example input sequence\n"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:41:14.593647Z","iopub.status.busy":"2023-11-15T13:41:14.593264Z","iopub.status.idle":"2023-11-15T13:41:14.628208Z","shell.execute_reply":"2023-11-15T13:41:14.627341Z","shell.execute_reply.started":"2023-11-15T13:41:14.593598Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /usr/share/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package universal_tagset to\n","[nltk_data]     /usr/share/nltk_data...\n","[nltk_data]   Package universal_tagset is already up-to-date!\n","[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["from tqdm import tqdm\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu\n","# from nltk.translate.meteor_score import meteor_score\n","# from nltk.translate.rouge import Rouge\n","\n","# Download necessary resources for nltk (run this once)\n","nltk.download(\"wordnet\")\n","nltk.download(\"averaged_perceptron_tagger\")\n","nltk.download(\"universal_tagset\")\n","nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:41:14.629957Z","iopub.status.busy":"2023-11-15T13:41:14.629533Z","iopub.status.idle":"2023-11-15T13:41:14.634816Z","shell.execute_reply":"2023-11-15T13:41:14.633861Z","shell.execute_reply.started":"2023-11-15T13:41:14.629922Z"},"trusted":true},"outputs":[],"source":["c = c_train[:500]\n","q = q_train[:500]\n","a = a_train[:500]"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:41:14.636400Z","iopub.status.busy":"2023-11-15T13:41:14.636075Z","iopub.status.idle":"2023-11-15T14:47:04.287343Z","shell.execute_reply":"2023-11-15T14:47:04.285416Z","shell.execute_reply.started":"2023-11-15T13:41:14.636365Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.16.0 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.15.12"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/kaggle/working/wandb/run-20231115_134238-t5v6ipn3</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/anlp_sarcasmdetection/SoftPrompt%20on%20GPT-2%20for%20SQUAD/runs/t5v6ipn3' target=\"_blank\">2</a></strong> to <a href='https://wandb.ai/anlp_sarcasmdetection/SoftPrompt%20on%20GPT-2%20for%20SQUAD' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/anlp_sarcasmdetection/SoftPrompt%20on%20GPT-2%20for%20SQUAD' target=\"_blank\">https://wandb.ai/anlp_sarcasmdetection/SoftPrompt%20on%20GPT-2%20for%20SQUAD</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/anlp_sarcasmdetection/SoftPrompt%20on%20GPT-2%20for%20SQUAD/runs/t5v6ipn3' target=\"_blank\">https://wandb.ai/anlp_sarcasmdetection/SoftPrompt%20on%20GPT-2%20for%20SQUAD/runs/t5v6ipn3</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Epoch 1:  18%|█▊        | 15317/86821 [1:03:52<4:58:09,  4.00it/s, loss=0.0863]\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[25], line 31\u001b[0m\n\u001b[1;32m     27\u001b[0m c_train, q_train, a_train \u001b[38;5;241m=\u001b[39m c_train\u001b[38;5;241m.\u001b[39mto(device), q_train\u001b[38;5;241m.\u001b[39mto(device), a_train\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Assuming you have a soft_prompt_ids for each training instance\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# If not, you might need to modify this part accordingly\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoft_prompt_c\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msoft_prompt_q\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, a_train)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","Cell \u001b[0;32mIn[21], line 49\u001b[0m, in \u001b[0;36mGPT2WithPromptTuning.forward\u001b[0;34m(self, input_c, input_q, soft_prompt_c, soft_prompt_q)\u001b[0m\n\u001b[1;32m     46\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([soft_prompt_c, c_embeddings ,soft_prompt_q, q_embeddings], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Pass the concatenated embeddings through the GPT-2 model\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt2_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:1074\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1066\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;124;03m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;124;03m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1072\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1074\u001b[0m transformer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1083\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1089\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m transformer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:888\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    876\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    877\u001b[0m         block\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    878\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    885\u001b[0m         output_attentions,\n\u001b[1;32m    886\u001b[0m     )\n\u001b[1;32m    887\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 888\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    899\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:390\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    388\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    389\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 390\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    399\u001b[0m outputs \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m1\u001b[39m:]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:331\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    329\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_upcast_and_reordered_attn(query, key, value, attention_mask, head_mask)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 331\u001b[0m     attn_output, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_merge_heads(attn_output, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    334\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(attn_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py:201\u001b[0m, in \u001b[0;36mGPT2Attention._attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    198\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfinfo(attn_weights\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mmin\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\u001b[39;00m\n\u001b[0;32m--> 201\u001b[0m     mask_value \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    202\u001b[0m     attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(causal_mask, attn_weights\u001b[38;5;241m.\u001b[39mto(attn_weights\u001b[38;5;241m.\u001b[39mdtype), mask_value)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from tqdm import tqdm\n","import wandb\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Move model to GPU\n","model.to(device)\n","\n","# Define device (assuming you have a CUDA-enabled GPU)\n","\n","# Move optimizer and criterion to GPU\n","optimizer = torch.optim.AdamW(model.soft_prompt_embeddings.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss(ignore_index=-100).to(device)\n","\n","# Initialize Weights and Biases\n","wandb.init(project='SoftPrompt on GPT-2 for SQUAD', name='2', config={'learning_rate': learning_rate})\n","\n","# Training loop\n","for epoch in range(epochs):\n","    # Create a tqdm progress bar for the training data\n","    data_iterator = tqdm(zip(c_train, q_train, a_train), desc=f'Epoch {epoch + 1}', total=len(c_train))\n","    \n","    for c_train, q_train, a_train in data_iterator:\n","        optimizer.zero_grad()\n","\n","        # Move input and target tensors to GPU\n","        c_train, q_train, a_train = c_train.to(device), q_train.to(device), a_train.to(device)\n","        \n","        # Assuming you have a soft_prompt_ids for each training instance\n","        # If not, you might need to modify this part accordingly\n","        outputs = model(c_train, q_train, soft_prompt_c.to(device), soft_prompt_q.to(device))\n","        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n","\n","        loss = criterion(logits, a_train)\n","        loss.backward()\n","\n","        # Gradient clipping to prevent exploding gradients\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_value)\n","\n","        optimizer.step()\n","\n","        # Update the progress bar description with the current loss\n","        data_iterator.set_postfix(loss=loss.item())\n","\n","#     # Validation loop\n","    model.eval()\n","    val_losses = []\n","    with torch.no_grad():\n","        for input_ids_val, target_ids_val in zip(input_ids_val, target_ids_val):\n","            input_ids_val, target_ids_val = input_ids_val.to(device), target_ids_val.to(device)\n","            outputs_val = model(input_ids_val, soft_prompt_ids.to(device))\n","            logits_val = outputs_val.logits if hasattr(outputs_val, \"logits\") else outputs_val.last_hidden_state\n","            loss_val = criterion(logits_val, target_ids_val)\n","            val_losses.append(loss_val.item())\n","            # Convert tensor predictions and references to lists\n","            predictions = logits_val.argmax(dim=-1).squeeze(0).tolist()\n","            references = target_ids_val.squeeze(0).tolist()\n","\n","            # BLEU Score\n","            bleu_score = sentence_bleu([references], predictions)\n","            print(f\"BLEU Score: {bleu_score}\")\n","    avg_val_loss = sum(val_losses) / len(val_losses)\n","    wandb.log({\"epoch\": epoch + 1, \"train_loss\": loss.item()})\n","\n","    # Set the model back to training mode\n","#     model.train()\n","\n","# Close the tqdm progress bar\n","data_iterator.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
