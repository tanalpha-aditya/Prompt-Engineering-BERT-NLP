{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-11-15T13:01:13.577951Z","iopub.status.busy":"2023-11-15T13:01:13.577433Z","iopub.status.idle":"2023-11-15T13:01:14.161514Z","shell.execute_reply":"2023-11-15T13:01:14.159882Z","shell.execute_reply.started":"2023-11-15T13:01:13.577914Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv\n","/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv\n","/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful| packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:01:14.165028Z","iopub.status.busy":"2023-11-15T13:01:14.164474Z","iopub.status.idle":"2023-11-15T13:01:33.918109Z","shell.execute_reply":"2023-11-15T13:01:33.916794Z","shell.execute_reply.started":"2023-11-15T13:01:14.164994Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import GPT2Model, GPT2Tokenizer, GPT2Config, AdamW\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:01:33.920626Z","iopub.status.busy":"2023-11-15T13:01:33.919736Z","iopub.status.idle":"2023-11-15T13:02:10.309339Z","shell.execute_reply":"2023-11-15T13:02:10.307046Z","shell.execute_reply.started":"2023-11-15T13:01:33.920583Z"},"trusted":true},"outputs":[],"source":["import csv\n","\n","# Assuming your CSV file is named 'your_dataset.csv'\n","csv_file_path = '/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/test.csv'\n","\n","# List to store tuples of (article, highlights)\n","test_data_list = []\n","\n","# Read CSV file and extract relevant columns\n","with open(csv_file_path, 'r', encoding='utf-8') as file:\n","    csv_reader = csv.DictReader(file)\n","    for row in csv_reader:\n","        article = row.get('article', '')\n","        highlights = row.get('highlights', '')\n","        test_data_list.append((article, highlights))\n","        \n","\n","# Assuming your CSV file is named 'your_dataset.csv'\n","csv_file_path = '/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/train.csv'\n","\n","# List to store tuples of (article, highlights)\n","train_data_list = []\n","\n","# Read CSV file and extract relevant columns\n","with open(csv_file_path, 'r', encoding='utf-8') as file:\n","    csv_reader = csv.DictReader(file)\n","    for row in csv_reader:\n","        article = row.get('article', '')\n","        highlights = row.get('highlights', '')\n","        train_data_list.append((article, highlights))\n","        \n","        \n","\n","# Assuming your CSV file is named 'your_dataset.csv'\n","csv_file_path = '/kaggle/input/newspaper-text-summarization-cnn-dailymail/cnn_dailymail/validation.csv'\n","\n","# List to store tuples of (article, highlights)\n","val_data_list = []\n","\n","# Read CSV file and extract relevant columns\n","with open(csv_file_path, 'r', encoding='utf-8') as file:\n","    csv_reader = csv.DictReader(file)\n","    for row in csv_reader:\n","        article = row.get('article', '')\n","        highlights = row.get('highlights', '')\n","        val_data_list.append((article, highlights))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:02:10.312445Z","iopub.status.busy":"2023-11-15T13:02:10.311789Z","iopub.status.idle":"2023-11-15T13:02:10.333025Z","shell.execute_reply":"2023-11-15T13:02:10.330636Z","shell.execute_reply.started":"2023-11-15T13:02:10.312395Z"},"trusted":true},"outputs":[],"source":["import random\n","# Set the seed for reproducibility\n","random.seed(42)  # You can choose any seed value\n","\n","# Calculate 1% of the original data size\n","one_percent_size = int(0.01 * len(test_data_list))\n","\n","# Randomly sample 1% of the data\n","onetest_data_list = random.sample(test_data_list, one_percent_size)\n","# Calculate 1% of the original data size\n","one_percent_size = int(0.01 * len(train_data_list))\n","\n","# Randomly sample 1% of the data\n","onetrain_data_list = random.sample(train_data_list, one_percent_size)\n","# Calculate 1% of the original data size\n","one_percent_size = int(0.01 * len(val_data_list))\n","\n","# Randomly sample 1% of the data\n","oneval_data_list = random.sample(val_data_list, one_percent_size)\n"]},{"cell_type":"markdown","metadata":{},"source":["DATA PREPROCESSING"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:02:10.342610Z","iopub.status.busy":"2023-11-15T13:02:10.341393Z","iopub.status.idle":"2023-11-15T13:02:12.440955Z","shell.execute_reply":"2023-11-15T13:02:12.438308Z","shell.execute_reply.started":"2023-11-15T13:02:10.342548Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.tokenize import word_tokenize\n","import string\n","\n","# Download NLTK resources (if not already downloaded)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Function for text preprocessing\n","def preprocess_text(text):\n","    # Convert to lowercase\n","    text = text.lower()\n","\n","    # Remove punctuation and digits\n","    text = text.translate(str.maketrans('', '', string.punctuation + string.digits))\n","\n","    # Tokenization\n","    tokens = word_tokenize(text)\n","\n","    # Remove stop words\n","    stop_words = set(stopwords.words('english'))\n","    tokens = [word for word in tokens if word not in stop_words]\n","\n","    # Lemmatization\n","#     lemmatizer = WordNetLemmatizer()\n","#     tokens = [lemmatizer.lemmatize(word) for word in tokens]\n","\n","    # Join the tokens back into a string\n","    preprocessed_text = ' '.join(tokens)\n","\n","    return preprocessed_text\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:02:12.444354Z","iopub.status.busy":"2023-11-15T13:02:12.443289Z","iopub.status.idle":"2023-11-15T13:02:36.622256Z","shell.execute_reply":"2023-11-15T13:02:36.620829Z","shell.execute_reply.started":"2023-11-15T13:02:12.444301Z"},"trusted":true},"outputs":[],"source":["\n","# Apply preprocessing to your data\n","ptrain_data_list = [(preprocess_text(article), preprocess_text(highlights)) for article, highlights in onetrain_data_list]\n","ptest_data_list = [(preprocess_text(article), preprocess_text(highlights)) for article, highlights in onetest_data_list]\n","pval_data_list = [(preprocess_text(article), preprocess_text(highlights)) for article, highlights in oneval_data_list]\n","\n","# Print the preprocessed result for the first entry\n"]},{"cell_type":"markdown","metadata":{},"source":["tokenize and convert to indices"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:02:36.624945Z","iopub.status.busy":"2023-11-15T13:02:36.624476Z","iopub.status.idle":"2023-11-15T13:02:59.891579Z","shell.execute_reply":"2023-11-15T13:02:59.890306Z","shell.execute_reply.started":"2023-11-15T13:02:36.624914Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7cef8997991a45fd91df62a792baba6c","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a96f2bfeb0cb4e2d9eb642bbbed7945f","version_major":2,"version_minor":0},"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e368d47ddea45529e3d2ec0b17090b5","version_major":2,"version_minor":0},"text/plain":["Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96441d382384490b955b08cb0858c396","version_major":2,"version_minor":0},"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1046 > 1024). Running this sequence through the model will result in indexing errors\n"]}],"source":["from transformers import GPT2Tokenizer\n","\n","# Load GPT-2 tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","# Define a function to tokenize, convert text to indices, and truncate to the first 1021 tokens\n","def tokenize_and_convert_to_indices(data_list, max_length=1021):\n","    tokenized_data_list = []\n","    for article, highlights in data_list:\n","        # Tokenize and convert to indices\n","        article_tokens = tokenizer.encode(article, add_special_tokens=True)\n","        highlights_tokens = tokenizer.encode(highlights, add_special_tokens=True)\n","\n","        # Truncate to the first 1021 tokens\n","        article_tokens = article_tokens[:max_length]\n","        highlights_tokens = highlights_tokens[:max_length+3]\n","\n","        # Append to the tokenized_data_list\n","        tokenized_data_list.append((article_tokens, highlights_tokens))\n","\n","    return tokenized_data_list\n","\n","# Apply tokenization and truncation to your datasets\n","max_length = 1021\n","tokenized_train_data_list = tokenize_and_convert_to_indices(ptrain_data_list, max_length)\n","tokenized_test_data_list = tokenize_and_convert_to_indices(ptest_data_list, max_length)\n","tokenized_val_data_list = tokenize_and_convert_to_indices(pval_data_list, max_length)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:02:59.893696Z","iopub.status.busy":"2023-11-15T13:02:59.893298Z","iopub.status.idle":"2023-11-15T13:02:59.901784Z","shell.execute_reply":"2023-11-15T13:02:59.898390Z","shell.execute_reply.started":"2023-11-15T13:02:59.893666Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["None\n"]}],"source":["wow = tokenizer._pad_token\n","print(wow)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:02:59.905317Z","iopub.status.busy":"2023-11-15T13:02:59.904705Z","iopub.status.idle":"2023-11-15T13:03:24.844404Z","shell.execute_reply":"2023-11-15T13:03:24.843241Z","shell.execute_reply.started":"2023-11-15T13:02:59.905266Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1046 > 1024). Running this sequence through the model will result in indexing errors\n"]}],"source":["from transformers import GPT2Tokenizer\n","import torch\n","from torch.nn.utils.rnn import pad_sequence\n","\n","# Load GPT-2 tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","\n","# Define a pad token and add it to the tokenizer\n","pad_token = tokenizer.eos_token\n","tokenizer.add_tokens([pad_token])\n","\n","# Define a function to tokenize, convert text to indices, and pad sequences\n","def tokenize_and_pad(data_list, max_article_length=1021, max_highlights_length=1024):\n","    tokenized_data_list = []\n","    for article, highlights in data_list:\n","        # Tokenize and convert to indices\n","        article_tokens = tokenizer.encode(article, add_special_tokens=True)\n","        highlights_tokens = tokenizer.encode(highlights, add_special_tokens=True)\n","\n","        # Pad sequences to specified lengths\n","        padded_article_tokens = torch.tensor(article_tokens + [tokenizer.convert_tokens_to_ids(pad_token)] * (max_article_length - len(article_tokens)))\n","        padded_highlights_tokens = torch.tensor(highlights_tokens + [tokenizer.convert_tokens_to_ids(pad_token)] * (max_highlights_length - len(highlights_tokens)))\n","\n","        # Append to the tokenized_data_list only if both token lists are not empty\n","        if len(article_tokens) > 0 and len(highlights_tokens) > 0:\n","            tokenized_data_list.append((padded_article_tokens, padded_highlights_tokens))\n","\n","    return tokenized_data_list\n","\n","# Apply tokenization and padding to your datasets\n","max_article_length = 1021\n","max_highlights_length = 1024\n","tokenized_train_data_list = tokenize_and_pad(ptrain_data_list, max_article_length, max_highlights_length)\n","tokenized_test_data_list = tokenize_and_pad(ptest_data_list, max_article_length, max_highlights_length)\n","tokenized_val_data_list = tokenize_and_pad(pval_data_list, max_article_length, max_highlights_length)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:03:24.846727Z","iopub.status.busy":"2023-11-15T13:03:24.846236Z","iopub.status.idle":"2023-11-15T13:03:24.858992Z","shell.execute_reply":"2023-11-15T13:03:24.858035Z","shell.execute_reply.started":"2023-11-15T13:03:24.846687Z"},"trusted":true},"outputs":[{"data":{"text/plain":["1046"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["len(tokenized_train_data_list[16][0])"]},{"cell_type":"markdown","metadata":{},"source":["DEFINING TRAIN AND TEST SET"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:03:24.861188Z","iopub.status.busy":"2023-11-15T13:03:24.860286Z","iopub.status.idle":"2023-11-15T13:03:24.873344Z","shell.execute_reply":"2023-11-15T13:03:24.872337Z","shell.execute_reply.started":"2023-11-15T13:03:24.861132Z"},"trusted":true},"outputs":[],"source":["# # Assuming you have tokenized data like tokenized_train_data_list\n","# # tokenized_train_data_list = [(padded_article_tokens, padded_highlights_tokens), ...]\n","\n","# # Initialize lists to store input and target ids\n","# input_ids_train = []\n","# target_ids_train = []\n","\n","# # Specify maximum lengths\n","# max_article_length = 1021\n","# max_highlights_length = 1024\n","\n","# # Iterate through the tokenized data\n","# for padded_article_tokens, padded_highlights_tokens in tokenized_train_data_list:\n","#     # Truncate article tokens if greater than max_article_length\n","#     truncated_article_tokens = padded_article_tokens[:max_article_length]\n","\n","#     # Truncate highlights tokens if greater than max_highlights_length\n","#     truncated_highlights_tokens = padded_highlights_tokens[:max_highlights_length]\n","\n","#     # Append truncated article tokens to input_ids_train\n","#     input_ids_train.append(truncated_article_tokens)\n","\n","#     # Append truncated highlights tokens to target_ids_train\n","#     target_ids_train.append(truncated_highlights_tokens)\n","\n","# # Convert the lists to PyTorch tensors\n","# input_ids_train = torch.stack(input_ids_train)\n","# target_ids_train = torch.stack(target_ids_train)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:03:24.876169Z","iopub.status.busy":"2023-11-15T13:03:24.874677Z","iopub.status.idle":"2023-11-15T13:03:24.933617Z","shell.execute_reply":"2023-11-15T13:03:24.932555Z","shell.execute_reply.started":"2023-11-15T13:03:24.876129Z"},"trusted":true},"outputs":[],"source":["# Initialize lists to store input and target ids\n","input_ids_val = []\n","target_ids_val = []\n","\n","# Specify maximum lengths\n","max_article_length = 1021\n","max_highlights_length = 1024\n","\n","# Iterate through the tokenized data\n","for padded_article_tokens, padded_highlights_tokens in tokenized_val_data_list:\n","    # Truncate article tokens if greater than max_article_length\n","    truncated_article_tokens = padded_article_tokens[:max_article_length]\n","\n","    # Truncate highlights tokens if greater than max_highlights_length\n","    truncated_highlights_tokens = padded_highlights_tokens[:max_highlights_length]\n","\n","    # Append truncated article tokens to input_ids_train\n","    input_ids_val.append(truncated_article_tokens)\n","\n","    # Append truncated highlights tokens to target_ids_train\n","    target_ids_val.append(truncated_highlights_tokens)\n","\n","# Convert the lists to PyTorch tensors\n","input_ids_val = torch.stack(input_ids_val)\n","target_ids_val = torch.stack(target_ids_val)"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-11-15T13:03:24.935959Z","iopub.status.busy":"2023-11-15T13:03:24.935558Z","iopub.status.idle":"2023-11-15T13:03:26.043181Z","shell.execute_reply":"2023-11-15T13:03:26.040505Z","shell.execute_reply.started":"2023-11-15T13:03:24.935927Z"},"trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'target_ids_train' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtarget_ids_train\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n","\u001b[0;31mNameError\u001b[0m: name 'target_ids_train' is not defined"]}],"source":["len(target_ids_train[0])"]},{"cell_type":"markdown","metadata":{},"source":["TRIAL"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:03:26.044655Z","iopub.status.idle":"2023-11-15T13:03:26.045286Z","shell.execute_reply":"2023-11-15T13:03:26.045002Z","shell.execute_reply.started":"2023-11-15T13:03:26.044974Z"},"trusted":true},"outputs":[],"source":["\n","# Tokenize the word \"summarize\"\n","tokens = tokenizer.tokenize(\"summarize\")\n","print(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:03:26.047936Z","iopub.status.idle":"2023-11-15T13:03:26.048562Z","shell.execute_reply":"2023-11-15T13:03:26.048273Z","shell.execute_reply.started":"2023-11-15T13:03:26.048246Z"},"trusted":true},"outputs":[],"source":["# import torch\n","# import torch.nn as nn\n","# from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, AdamW\n","\n","# # Load GPT-2 model and tokenizer\n","# model_name = \"gpt2\"\n","# tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","# gpt2_model = GPT2LMHeadModel.from_pretrained(model_name)\n","\n","# # Define the number of prompts and embedding size\n","# num_prompts = 4  # \"summarize the following text\"\n","# embedding_size = 768\n","\n","# # Create an embedding layer for soft prompts\n","# soft_prompt_embeddings = nn.Embedding(num_prompts, embedding_size)\n","\n","# # Concatenate soft prompt embeddings at the beginning of the input sequence\n","# class GPT2WithPromptTuning(nn.Module):\n","#     def __init__(self, gpt2_model, soft_prompt_embeddings):\n","#         super(GPT2WithPromptTuning, self).__init__()\n","#         self.gpt2_model = gpt2_model\n","#         self.soft_prompt_embeddings = soft_prompt_embeddings\n","    \n","#     def forward(self, input_ids, soft_prompt_ids):\n","#         # Get the embeddings for the input_ids from the GPT-2 model\n","# #         gpt2_outputs = self.gpt2_model(input_ids)\n","# #         gpt2_embeddings = gpt2_outputs['last_hidden_state']\n","#         gpt2_embeddings = self.gpt2_model.transformer.wte(input_ids)\n","#         # Get the embeddings for the soft prompts\n","#         soft_prompt_embeds = self.soft_prompt_embeddings(soft_prompt_ids)\n","        \n","#         # Concatenate the embeddings\n","#         embeddings = torch.cat([soft_prompt_embeds, gpt2_embeddings], dim=0)\n","        \n","#         # Pass the concatenated embeddings through the GPT-2 model\n","#         outputs = self.gpt2_model(inputs_embeds=embeddings)\n","        \n","#         return outputs "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:03:26.050435Z","iopub.status.idle":"2023-11-15T13:03:26.051024Z","shell.execute_reply":"2023-11-15T13:03:26.050771Z","shell.execute_reply.started":"2023-11-15T13:03:26.050744Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","# Load GPT-2 model and tokenizer\n","model_name = \"gpt2\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n","gpt2_model = GPT2LMHeadModel.from_pretrained(model_name)\n","\n","# Define the number of prompts and embedding size\n","num_prompts = 3  # \"summarize the following text\"\n","embedding_size = 768\n","\n","# Define a specific sentence\n","sentence = \"summarize\"\n","# sum + are + rise\n","# Tokenize the sentence\n","input_ids = tokenizer.encode(sentence, return_tensors='pt')\n","\n","# Get the embeddings for the input_ids from the GPT-2 model\n","gpt2_embeddings = gpt2_model.transformer.wte(input_ids)\n","\n","# Create an embedding layer for soft prompts and initialize with the sentence embeddings\n","soft_prompt_embeddings = nn.Embedding(num_prompts, embedding_size)\n","soft_prompt_embeddings.weight.data.copy_(gpt2_embeddings.squeeze(0))\n","\n","# Concatenate soft prompt embeddings at the beginning of the input sequence\n","class GPT2WithPromptTuning(nn.Module):\n","    def __init__(self, gpt2_model, soft_prompt_embeddings):\n","        super(GPT2WithPromptTuning, self).__init__()\n","        self.gpt2_model = gpt2_model\n","        self.soft_prompt_embeddings = soft_prompt_embeddings\n","    \n","    def forward(self, input_ids, soft_prompt_ids):\n","        # Get the embeddings for the input_ids from the GPT-2 model\n","        gpt2_embeddings = self.gpt2_model.transformer.wte(input_ids)\n","        # Get the embeddings for the soft prompts\n","        soft_prompt_embeds = self.soft_prompt_embeddings(soft_prompt_ids)\n","        \n","        # Concatenate the embeddings\n","        embeddings = torch.cat([soft_prompt_embeds, gpt2_embeddings], dim=0)\n","        \n","        # Pass the concatenated embeddings through the GPT-2 model\n","        outputs = self.gpt2_model(inputs_embeds=embeddings)\n","        \n","        return outputs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:03:26.052742Z","iopub.status.idle":"2023-11-15T13:03:26.053349Z","shell.execute_reply":"2023-11-15T13:03:26.053058Z","shell.execute_reply.started":"2023-11-15T13:03:26.053029Z"},"trusted":true},"outputs":[],"source":["len(target_ids_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:03:26.056323Z","iopub.status.idle":"2023-11-15T13:03:26.056924Z","shell.execute_reply":"2023-11-15T13:03:26.056654Z","shell.execute_reply.started":"2023-11-15T13:03:26.056626Z"},"trusted":true},"outputs":[],"source":["# Initialize the model\n","model = GPT2WithPromptTuning(gpt2_model, soft_prompt_embeddings)\n","\n","# Freeze GPT-2 model weights\n","for param in model.gpt2_model.parameters():\n","    param.requires_grad = False\n","\n","# Define hyperparameters\n","batch_size = 8\n","epochs = 5\n","learning_rate = 2e-5\n","gradient_clip_value = 1.0\n","\n","# Define optimizer and criterion\n","optimizer = torch.optim.AdamW(model.soft_prompt_embeddings.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss(ignore_index=-100)\n","\n","# Example data (replace with your own dataset)\n","# input_ids = torch.randint(0, 100, (1023,))  # Example input sequence\n","soft_prompt_ids = torch.tensor([0, 1, 2])\n","# target_ids = torch.randint(0, 100, (1024,))  # Example input sequence\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:03:26.059328Z","iopub.status.idle":"2023-11-15T13:03:26.059819Z","shell.execute_reply":"2023-11-15T13:03:26.059620Z","shell.execute_reply.started":"2023-11-15T13:03:26.059601Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","import nltk\n","from nltk.translate.bleu_score import sentence_bleu\n","# from nltk.translate.meteor_score import meteor_score\n","# from nltk.translate.rouge import Rouge\n","\n","# Download necessary resources for nltk (run this once)\n","nltk.download(\"wordnet\")\n","nltk.download(\"averaged_perceptron_tagger\")\n","nltk.download(\"universal_tagset\")\n","nltk.download(\"punkt\")\n","nltk.download(\"stopwords\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-11-15T13:03:26.062189Z","iopub.status.idle":"2023-11-15T13:03:26.063377Z","shell.execute_reply":"2023-11-15T13:03:26.063029Z","shell.execute_reply.started":"2023-11-15T13:03:26.062995Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","import wandb\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Move model to GPU\n","model.to(device)\n","\n","# Define device (assuming you have a CUDA-enabled GPU)\n","\n","# Move optimizer and criterion to GPU\n","optimizer = torch.optim.AdamW(model.soft_prompt_embeddings.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss(ignore_index=-100).to(device)\n","\n","# Initialize Weights and Biases\n","wandb.init(project='SoftPrompt on GPT-2 for Summarization', name='1', config={'learning_rate': learning_rate})\n","\n","# Training loop\n","for epoch in range(epochs):\n","    # Create a tqdm progress bar for the training data\n","    data_iterator = tqdm(zip(input_ids_train, target_ids_train), desc=f'Epoch {epoch + 1}', total=len(input_ids_train))\n","    \n","    for input_ids, target_ids in data_iterator:\n","        optimizer.zero_grad()\n","\n","        # Move input and target tensors to GPU\n","        input_ids, target_ids = input_ids.to(device), target_ids.to(device)\n","        \n","        # Assuming you have a soft_prompt_ids for each training instance\n","        # If not, you might need to modify this part accordingly\n","        outputs = model(input_ids, soft_prompt_ids.to(device))\n","        logits = outputs.logits if hasattr(outputs, \"logits\") else outputs.last_hidden_state\n","\n","        loss = criterion(logits, target_ids) \n","        loss.backward()\n","\n","        # Gradient clipping to prevent exploding gradients\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_value)\n","\n","        optimizer.step()\n","\n","        # Update the progress bar description with the current loss\n","        data_iterator.set_postfix(loss=loss.item())\n","\n","#     # Validation loop\n","    model.eval()\n","    val_losses = []\n","    with torch.no_grad():\n","        for input_ids_val, target_ids_val in zip(input_ids_val, target_ids_val):\n","            input_ids_val, target_ids_val = input_ids_val.to(device), target_ids_val.to(device)\n","            outputs_val = model(input_ids_val, soft_prompt_ids.to(device))\n","            logits_val = outputs_val.logits if hasattr(outputs_val, \"logits\") else outputs_val.last_hidden_state\n","            loss_val = criterion(logits_val, target_ids_val)\n","            val_losses.append(loss_val.item())\n","            # Convert tensor predictions and references to lists\n","            predictions = logits_val.argmax(dim=-1).squeeze(0).tolist()\n","            references = target_ids_val.squeeze(0).tolist()\n","\n","            # BLEU Score\n","            bleu_score = sentence_bleu([references], predictions)\n","            print(f\"BLEU Score: {bleu_score}\")\n","    avg_val_loss = sum(val_losses) / len(val_losses)\n","    wandb.log({\"epoch\": epoch + 1, \"train_loss\": loss.item(), \"val_loss\": avg_val_loss})\n","\n","    # Set the model back to training mode\n","#     model.train()\n","\n","# Close the tqdm progress bar\n","data_iterator.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":1654566,"sourceId":2734496,"sourceType":"datasetVersion"}],"dockerImageVersionId":30579,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
